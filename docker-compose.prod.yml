services:
  app:
    image: ghcr.io/toosmooth/doogiebot:latest
    ports:
      - "3000:3000"  # Frontend
      - "8000:8000"  # Backend API
    volumes:
      - ./data/db:/app/backend/db  # Persist database files
      - ./data/indexes:/app/backend/indexes  # Persist index files
      - ./entrypoint.prod.sh:/app/entrypoint.prod.sh  # Mount the production entrypoint script
    environment:
      - NODE_ENV=production
      - PYTHONPATH=/app
      - FASTAPI_ENV=production
      - SECRET_KEY=${SECRET_KEY:-}  # Required in production
      - ALGORITHM=HS256
      - DISABLE_SQL_LOGS=true
      - ACCESS_TOKEN_EXPIRE_MINUTES=30
      # Admin credentials are now optional
      - FIRST_ADMIN_EMAIL=${FIRST_ADMIN_EMAIL:-}
      - FIRST_ADMIN_PASSWORD=${FIRST_ADMIN_PASSWORD:-}
      # LLM Service environment variables
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://localhost:11434}
      - LM_STUDIO_BASE_URL=${LM_STUDIO_BASE_URL:-http://localhost:8000}
    restart: always
    entrypoint: ["/app/entrypoint.prod.sh"]
    # Production-appropriate resource constraints
    deploy:
      resources:
        limits:
          memory: 6G
          cpus: '4'
        reservations:
          memory: 3G
          cpus: '2'
    # More strict healthcheck for production
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 1m
      timeout: 10s
      retries: 5
      start_period: 40s