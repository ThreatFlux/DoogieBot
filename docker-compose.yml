services:
  app:
    image: ghcr.io/toosmooth/doogiebot:latest
    ports:
      - "3000:3000"  # Frontend
      - "8000:8000"  # Backend API
    volumes:
      # Use more specific bind mounts for development
      - ./backend:/app/backend  # Bind mount for backend development
      - ./frontend:/app/frontend  # Bind mount for frontend development
      - /var/run/docker.sock:/var/run/docker.sock  # Mount Docker socket for Docker-in-Docker (MCP servers)
      - ./data/db:/app/backend/db  # Persist database files
      - ./data/indexes:/app/backend/indexes  # Persist index files
      - ./entrypoint.sh:/app/entrypoint.sh  # Mount the entrypoint script
      # Exclude node_modules and build artifacts from host machine
      #- /app/frontend/node_modules
      #- /app/frontend/.next
      #- /app/backend/__pycache__
    environment:
      - NODE_ENV=development
      - PYTHONPATH=/app
      - FASTAPI_ENV=development
      - SECRET_KEY=${SECRET_KEY:-your-secret-key-change-in-production}
      - ALGORITHM=HS256
      - DISABLE_SQL_LOGS=true
      - ACCESS_TOKEN_EXPIRE_MINUTES=30
      - FIRST_ADMIN_EMAIL=${FIRST_ADMIN_EMAIL:-admin@example.com}
      - FIRST_ADMIN_PASSWORD=${FIRST_ADMIN_PASSWORD:-change-this-password}
      # LLM Service environment variables
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://localhost:11434}
      - LM_STUDIO_BASE_URL=${LM_STUDIO_BASE_URL:-http://localhost:8000}
      # Memory management
      - PYTHONMALLOC=debug
      - PYTHONWARNINGS=always
      # MCP configuration
      - MCP_NETWORK=mcp-network
      - MCP_DATA_DIR=/var/lib/doogie-chat/mcp
      - MCP_ENABLE_DOCKER=true
    restart: unless-stopped
    entrypoint: ["/app/entrypoint.sh"]
    # Add memory limits to prevent OOM issues
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'
        reservations:
          memory: 2G
          cpus: '1'
    # Add healthcheck
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s